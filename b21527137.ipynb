{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynFEQ3HKwXxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmfXVA4kysOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "import sklearn.svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "if use_gpu:\n",
        "    print(\"CUDA kullan覺l覺yor\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG9a0893RReo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "     'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "data_transformsInception = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(299),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(299),\n",
        "        transforms.CenterCrop(299),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "     'test': transforms.Compose([\n",
        "        transforms.Resize(299),\n",
        "        transforms.CenterCrop(299),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "data_dir = '/content/drive/My Drive/datasetnoisy'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val','test']}\n",
        "image_datasetInception = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transformsInception[x])\n",
        "                  for x in ['train', 'val','test']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train','val', 'test']}\n",
        "dataloaderInception =  {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train','val', 'test']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val','test']}\n",
        "class_names = image_datasets['train'].classes\n",
        "for x in [\"train\", \"val\", \"test\"]:\n",
        "    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n",
        "print(image_datasets[\"train\"].classes)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCiw31UlWqHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(inp, title=None):\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    #plt.figure(figsize=(10, 10))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "def show_databatch(inputs, classes):\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "    imshow(out, title = [class_names[x] for x in classes])\n",
        "inputs, classes = next(iter(dataloaders[\"train\"]))\n",
        "show_databatch(inputs,classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTxEcXVOIPXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['test']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title(\"predicted: {} actual: {}\".format(class_names[preds[j]],class_names[labels.data.cpu().numpy()[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j93s5_oglL1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_model(vgg, criterion,dtldr):\n",
        "    since = time.time()\n",
        "    avg_loss = 0\n",
        "    avg_acc = 0\n",
        "    loss_test = 0\n",
        "    acc_test = 0\n",
        "    test_batches = len(dtldr[\"test\"])\n",
        "    print(\"Hesaplama modeli\")\n",
        "    print('-' * 10)\n",
        "    for i, data in enumerate(dtldr[\"test\"]):\n",
        "        print(\"\\rTest batch {}/{}\".format(i, test_batches), end = '', flush=True)\n",
        "        \n",
        "        vgg.train(False)\n",
        "        vgg.eval()\n",
        "        inputs, labels = data\n",
        "        \n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = vgg(inputs)\n",
        "        \n",
        "        _, preds = torch.max(outputs.data.cpu(), 1)\n",
        "        loss = criterion(outputs, labels)       \n",
        "        loss_test += loss.data\n",
        "        acc_test += torch.sum(preds.cpu() == labels.data.cpu())\n",
        "        \n",
        "        del inputs, labels, outputs, preds\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    \n",
        "    avg_loss = loss_test / dataset_sizes[\"test\"]\n",
        "    avg_acc = acc_test.item() / dataset_sizes[\"test\"]\n",
        "    \n",
        "    elapsed_time = time.time() - since\n",
        "    print()\n",
        "    print(\"Hesaplama {:.0f}m {:.0f}s zamanda tamamland覺\".format(elapsed_time // 60, elapsed_time % 60))\n",
        "    print(\"Ortalama kay覺p (test): {:.4f}\".format(avg_loss))\n",
        "    print(\"Ortalama acc (test): {:.4f}\".format(avg_acc))\n",
        "    print('-' * 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzQ-yv_zSCxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(vgg, criterion, optimizer, scheduler, num_epochs,dtldr):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(vgg.state_dict())\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    avg_loss = 0\n",
        "    avg_acc = 0\n",
        "    avg_loss_val = 0\n",
        "    avg_acc_val = 0\n",
        "    loss_graph_train = []\n",
        "    loss_graph_valid = []\n",
        "    \n",
        "    train_batches = len(dtldr[\"train\"])\n",
        "    val_batches = len(dtldr[\"val\"])\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "        \n",
        "        loss_train = 0\n",
        "        loss_val = 0\n",
        "        acc_train = 0\n",
        "        acc_val = 0\n",
        "        \n",
        "        vgg.train(True)\n",
        "        \n",
        "        for i, data in enumerate(dtldr[\"train\"]):\n",
        "            if i % 100 == 0:\n",
        "                print(\"\\rTraining batch {}/{}\".format(i, train_batches), end='', flush=True)\n",
        "                \n",
        "            inputs, labels = data\n",
        "            \n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "                \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = vgg(inputs)\n",
        "            \n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            loss_train += loss.data\n",
        "            acc_train += torch.sum(preds == labels.data).data.cpu().numpy()\n",
        "            \n",
        "            del inputs, labels, outputs, preds\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        print()\n",
        "        # * 2 as we only used half of the dataset\n",
        "        avg_loss = loss_train  / dataset_sizes[\"train\"]\n",
        "        avg_acc = acc_train  / dataset_sizes[\"train\"]\n",
        "        loss_graph_train.append(avg_loss.data.cpu().item())\n",
        "        vgg.train(False)\n",
        "        vgg.eval()\n",
        "            \n",
        "        for i, data in enumerate(dtldr[\"val\"]):\n",
        "            if i % 100 == 0:\n",
        "                print(\"\\rValidation batch {}/{}\".format(i, val_batches), end='', flush=True)\n",
        "                \n",
        "            inputs, labels = data\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = vgg(inputs)\n",
        "            \n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            loss_val += loss.data\n",
        "            acc_val += torch.sum(preds == labels.data).data.cpu().numpy()\n",
        "            del inputs, labels, outputs, preds\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        avg_loss_val = loss_val / dataset_sizes[\"val\"]\n",
        "        avg_acc_val = acc_val / dataset_sizes[\"val\"]\n",
        "        \n",
        "        loss_graph_valid.append(avg_loss_val.data.cpu().item())\n",
        "        \n",
        "        print()\n",
        "        print(\"Epoch {} result: \".format(epoch))\n",
        "        print(\"Avg loss (train): {:.4f}\".format(avg_loss))\n",
        "        print(\"Avg acc (train): {:.4f}\".format(avg_acc))\n",
        "        print(\"Avg loss (val): {:.4f}\".format(avg_loss_val))\n",
        "        print(\"Avg acc (val): {:.4f}\".format(avg_acc_val))\n",
        "        print('-' * 10)\n",
        "        print()\n",
        "        \n",
        "        if avg_acc_val > best_acc:\n",
        "            best_acc = avg_acc_val\n",
        "            best_model_wts = copy.deepcopy(vgg.state_dict())\n",
        "        \n",
        "    elapsed_time = time.time() - since\n",
        "    print()\n",
        "    print(\"Training completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
        "    print(\"Best acc: {:.4f}\".format(best_acc))\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    epochs = []\n",
        "    for i in range(num_epochs):\n",
        "      epochs.append(i+1)\n",
        "    plot1, = plt.plot(np.array(epochs),np.array(loss_graph_train))\n",
        "    plot2, = plt.plot(np.array(epochs),np.array(loss_graph_valid))\n",
        "    plt.legend([plot1, plot2], [\"train_loss\", \"validation_loss\"])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Average Negative Log Likelihood')\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.show()\n",
        "    vgg.load_state_dict(best_model_wts)\n",
        "    return vgg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLZQqjEEVEJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet18 = models.resnet18(pretrained=True)\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "densenet = models.densenet121(pretrained=True)\n",
        "inception = models.inception_v3(pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSlqcguAVJHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet18.fc = nn.Linear(512, len(class_names))\n",
        "alexnet.classifier[6] = nn.Linear(4096,len(class_names))\n",
        "num_features = vgg16.classifier[6].in_features\n",
        "features = list(vgg16.classifier.children())[:-1] \n",
        "features.extend([nn.Linear(num_features, len(class_names))])\n",
        "vgg16.classifier = nn.Sequential(*features)\n",
        "num_ftrs = densenet.classifier.in_features\n",
        "densenet.classifier = nn.Linear(num_ftrs, len(class_names))\n",
        "inception.AuxLogits.fc = nn.Linear(768,len(class_names))\n",
        "inception.fc = nn.Linear(2048,len(class_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je4HcXGusE1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet18 = resnet18.to(device)\n",
        "alexnet = alexnet.to(device)\n",
        "vgg16 = vgg16.to(device)\n",
        "densenet = densenet.to(device)\n",
        "inception = inception.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa0b-MdknZky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(resnet18)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJlehLPuj0WT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"VGG16\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.SGD(vgg16.parameters(), lr = 0.0001)\n",
        "# optimizerSGD4 = optim.SGD(vgg16.parameters(), lr = 0.00001)\n",
        "# optimizerAdam3 = optim.Adam(vgg16.parameters(), lr = 0.0001)\n",
        "# optimizerSGD3 = optim.SGD(vgg16.parameters(), lr = 0.0001)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size = 4, gamma = 0.1)\n",
        "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size = 4, gamma = 0.1)\n",
        "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size = 4, gamma = 0.1)\n",
        "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size = 4, gamma = 0.1)\n",
        "vgg16 = train_model(vgg16, criterion, optimizer_ft, exp_lr_scheduler, 20,dataloaders)\n",
        "torch.save(vgg16.state_dict(), 'VGG16_FineTune.pt')\n",
        "eval_model(vgg16, criterion,dataloaders)\n",
        "\n",
        "print(\"ResNet18\")\n",
        "\n",
        "optimizer_ft = optim.SGD(resnet18.parameters(), lr = 0.0001)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size = 4, gamma = 0.1)\n",
        "resnet18 = train_model(resnet18,criterion,optimizer_ft,exp_lr_scheduler,20,dataloaders)\n",
        "torch.save(resnet18.state_dict(), 'ResNet18_FineTune.pt')\n",
        "eval_model(resnet18, criterion,dataloaders)\n",
        "\n",
        "print(\"AlexNet\")\n",
        "\n",
        "optimizer_ft = optim.SGD(alexnet.parameters(), lr = 0.0001)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size = 4, gamma = 0.1)\n",
        "alexnet = train_model(alexnet, criterion, optimizer_ft, exp_lr_scheduler, 20,dataloaders)\n",
        "torch.save(alexnet.state_dict(), 'AlexNet_FineTune.pt')\n",
        "eval_model(alexnet, criterion,dataloaders)\n",
        "\n",
        "\n",
        "print(\"DenseNet\")\n",
        "optimizer_ft = optim.SGD(densenet.parameters(), lr = 0.0001)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size = 4, gamma = 0.1)\n",
        "densenet = train_model(densenet, criterion, optimizer_ft, exp_lr_scheduler, 20,dataloaders)\n",
        "torch.save(densenet.state_dict(), 'denseNet_FineTune.pt')\n",
        "eval_model(densenet, criterion , dataloaders)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7Ct4Kv_gZzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize_model(alexnet,4)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}